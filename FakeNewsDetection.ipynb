{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejYJSgq_EUjd",
        "outputId": "96e016e9-8c76-4d1b-c3e9-3a1a5d855b1a"
      },
      "outputs": [],
      "source": [
        "# Step 1: Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('punkt')     # Download punkt\n",
        "nltk.download('stopwords') # Also download stopwords\n",
        "\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "import joblib\n",
        "\n",
        "\n",
        "\n",
        "# Step 2: Load Two Datasets\n",
        "df1 = pd.read_csv('/content/Fake.csv')   # Replace with your path\n",
        "df2 = pd.read_csv('/content/True.csv')   # Replace with your path\n",
        "\n",
        "# Step 3: Preprocessing - Make Columns Consistent if Needed\n",
        "# Example only if your second dataset has different column names\n",
        "# df2.rename(columns={'headline': 'title', 'article': 'text', 'tag': 'label'}, inplace=True)\n",
        "\n",
        "# Step 4: Combine Two Datasets\n",
        "df = pd.concat([df1, df2], ignore_index=True)\n",
        "\n",
        "# Step 5: Shuffle Combined Dataset\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(\"Combined dataset shape:\", df.shape)\n",
        "print(df.head())\n",
        "\n",
        "# Step 6: Text Cleaning Function\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub('<.*?>', '', text)  # Remove HTML\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)  # Remove punctuation\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)  # Remove words with numbers\n",
        "    words = text.split()  # SIMPLE split on space (no word_tokenize)\n",
        "    words = [word for word in words if word not in stopwords.words('english')]\n",
        "    return \" \".join(words)\n",
        "\n",
        "# Step 7: Apply Cleaning\n",
        "df['clean_text'] = df['text'].apply(clean_text)\n",
        "\n",
        "# Step 8: Encode Labels\n",
        "df['label'] = df['label'].map({'REAL':1, 'FAKE':0})  # Map labels to 1 and 0\n",
        "print(df['label'].value_counts())\n",
        "\n",
        "# Step 9: Split into Features and Target\n",
        "X = df['clean_text']\n",
        "y = df['label']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 10: Feature Extraction - TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Step 11: Build Logistic Regression Model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Step 12: Model Evaluation\n",
        "y_pred = model.predict(X_test_tfidf)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Step 13: Save the Model and TF-IDF Vectorizer\n",
        "joblib.dump(model, 'fake_news_model.pkl')\n",
        "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.pkl')\n",
        "\n",
        "# Step 14: Create a Prediction Function\n",
        "def predict_news(news_text):\n",
        "    cleaned = clean_text(news_text)\n",
        "    vectorized = tfidf_vectorizer.transform([cleaned])\n",
        "    prediction = model.predict(vectorized)\n",
        "    return 'REAL' if prediction[0] == 1 else 'FAKE'\n",
        "\n",
        "# Example prediction\n",
        "sample_news = \"NASA has confirmed a new planet discovered by James Webb Telescope.\"\n",
        "print(\"\\nPrediction:\", predict_news(sample_news))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6dktKPZEZSZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
